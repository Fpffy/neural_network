{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handwritten Digit Recognition with Neural Network<br><br>\n",
    "\n",
    "<img src='drawings/title.jpg'></img>\n",
    "\n",
    "####  Objective:\n",
    "The goal of this project is to implement dense neural network and train it on the MNIST database to recognize Handwritten digits. Network will be able to have any number of layers and any number of neurons in each layer. Implementation will be done from scratch, without deep learning frameworks.\n",
    "\n",
    "#### Content:\n",
    "\n",
    "* [1](#1) Dataset<br>\n",
    "    * [1.1](#1) Description of data<br>\n",
    "    * [1.2](#1.2) Data preprocessing<br>\n",
    "    * [1.3](#1.3) Final data summary<br>\n",
    "* [2](#2) Utilities functions<br>\n",
    "    * [2.1](#2) Spliting data into mini batches<br>\n",
    "    * [2.2](#2.2) Create one hot vector<br>\n",
    "    * [2.3](#2.3) Initialize parameters<br>\n",
    "    * [2.4](#2.4) Network prediction<br>\n",
    "    * [2.5](#2.5) Measure accurancy<br>\n",
    "    * [2.6](#2.6) Activation functions<br>\n",
    "        * [2.6.1](#2.6) Hidden layers activation - ReLU<br>\n",
    "        * [2.6.2](#2.6.2) Output layer activation - Softmax<br>\n",
    "        * [2.6.3](#2.6.3) ReLU for backward propagation<br>\n",
    "* [3](#3) Model<br>\n",
    "    * [3.1](#3.1) Choosing neural network architecture<br>\n",
    "    * [3.2](#3.2) Parameters initialization<br>\n",
    "    * [3.3](#3.3) Forward propagation<br>\n",
    "    * [3.4](#3.4) Compute cost<br>\n",
    "    * [3.5](#3.5) Backward propagation<br>\n",
    "    * [3.6](#3.6) Update parameters (Adam optimizer)<br>\n",
    "        * [3.6.1](#3.6.1) Parameters initialization for Adam optimizer<br>\n",
    "    * [3.7](#3.7) Final model function<br>\n",
    "* [4](#4) Training<br>\n",
    "    * [4.1](#4) Hyperparameters<br>\n",
    "    * [4.2](#4.2) Model performace<br>\n",
    "* [5](#5) Image prediction<br>\n",
    "    * [5.1](#5.1) Example<br>\n",
    "* [6](#6) Conclusion<br><br>\n",
    "\n",
    "#### Importing necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from skimage.transform import resize\n",
    "import imageio\n",
    "import time\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1'></a>\n",
    "## 1. Dataset\n",
    "### 1.1 Description of data\n",
    "\n",
    "The data for this project is wildly avalible MNIST handwritten digit database. It has a training set of 60,000 examples,\n",
    "and a test set of 10,000 examples. Train set will be used to fit model, and test set will be used for model evalutation to find out for example if it is overfitting. Data is saved in CSV format and have six files:\n",
    "\n",
    "```\n",
    " - mnist_train_part_1.csv\n",
    " - mnist_train_part_2.csv\n",
    " - mnist_train_part_3.csv\n",
    " - mnist_train_part_4.csv\n",
    " - mnist_train_part_5.csv\n",
    " - mnist_test.csv\n",
    "```\n",
    "Format of data in each file is the same and it is shown in table bellow:\n",
    "\n",
    "| label |pix-1x1|pix-1x2|pix-1x3| ... |pix-28x26|pix-28x27|pix-28x28|\n",
    "|-------|-------|-------|-------|-----|---------|---------|---------|\n",
    "|  0-9  | 0-255 | 0-255 | 0-255 |   ..| 0-255   |   0-255 | 0-255   |\n",
    "|  0-9  | 0-255 | 0-255 | 0-255 |   ..| 0-255   |   0-255 | 0-255   |\n",
    "|  0-9  | 0-255 | 0-255 | 0-255 |   ..| 0-255   |   0-255 | 0-255   |\n",
    "|  ...  | ...   |   ... |  ...  |   ..| ...     |   ...   | ...     |\n",
    "\n",
    "* Each row is a new training example.\n",
    "* First column is a true label for example.\n",
    "* Each example is a $28x28$ image of handwritten digit in grayscale.\n",
    "* Pixels of each inmage are unrolled into vector of shape $785x1$.\n",
    "* $pix\\!-\\!ixj$ is the pixel value in $ith$ row and $jth$ column.\n",
    "* Values of pixels are between $0$ and $255$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading dataset from csv format to numpy.ndarray\n",
    "train_data_part_1 = pd.read_csv('datasets/mnist_train_part_1.csv').values\n",
    "train_data_part_2 = pd.read_csv('datasets/mnist_train_part_2.csv').values\n",
    "train_data_part_3 = pd.read_csv('datasets/mnist_train_part_3.csv').values\n",
    "train_data_part_4 = pd.read_csv('datasets/mnist_train_part_4.csv').values\n",
    "train_data_part_5 = pd.read_csv('datasets/mnist_train_part_5.csv').values\n",
    "train_data = np.concatenate((train_data_part_1, train_data_part_2, train_data_part_3, train_data_part_4, train_data_part_5))\n",
    "test_data = pd.read_csv('datasets/mnist_test.csv').values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1.2'></a>\n",
    "### 1.2 Data preprocessing\n",
    "__Following steps was carried out in order to make data easier to use:__\n",
    "1. The order of the examples was randomly shuffled:\n",
    "```python\n",
    "X_train = train_data[:, 1:]\n",
    "```\n",
    "2. The data sets has been divided into seperate arrays where $X$ is array of examples and $Y$ is labels for examples:\n",
    "```python\n",
    "X_train = train_data[:, 1:]\n",
    "Y_train = train_data[:, 0]\n",
    "```\n",
    "3. Examples have been normalized by dividing each pixel value by 255, so all values are in the range of $0$ to $1$:\n",
    "```python\n",
    "X_train = X_train / 255\n",
    "```\n",
    "\n",
    "4. Shape of arrays was changed so examples are in columns and pixel values are in rows:\n",
    "```python\n",
    "X_train = np.transpose(X_train)\n",
    "Y_train = np.reshape(Y_train, (1, Y_train.shape[0]))\n",
    "```\n",
    "5. Data type of $Y$ labels was changed to ```int```:\n",
    "```python\n",
    "Y_train = Y_train.astype(np.int8)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(train_data)\n",
    "np.random.shuffle(test_data)\n",
    "\n",
    "X_train = train_data[:, 1:]\n",
    "X_train = np.transpose(X_train)\n",
    "X_train = X_train / 255\n",
    "\n",
    "Y_train = train_data[:, 0]\n",
    "Y_train = np.reshape(Y_train, (1, Y_train.shape[0]))\n",
    "Y_train = Y_train.astype(np.int8)\n",
    "\n",
    "X_test = test_data[:, 1:]\n",
    "X_test = np.transpose(X_test)\n",
    "X_test = X_test / 255\n",
    "\n",
    "Y_test = test_data[:, 0]\n",
    "Y_test = np.reshape(Y_test, (1, Y_test.shape[0]))\n",
    "Y_test = Y_test.astype(np.int8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1.3'></a>\n",
    "### 1.3 Final data summary\n",
    "Legend:\n",
    "\n",
    "* $x_{i}^{(j)}$ - value of pixel $jth$ in $ith$ training example\n",
    "* $y^{(i)}$ - label for $ith$ example\n",
    "\n",
    "\n",
    "<center>\n",
    "$X_{train}=\\begin\n",
    "{bmatrix}\n",
    "x_{1}^{(1)} & x_{1}^{(2)} & \\dots & x_{1}^{(60000)}\\\\\n",
    "x_{2}^{(1)} & x_{2}^{(2)} & \\dots & x_{2}^{(60000)}\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "x_{784}^{(1)} & x_{784}^{(2)} & \\dots & x_{784}^{(60000)}\n",
    "\\end{bmatrix}$\n",
    "$,\\quad$\n",
    "$Y_{train}=\\begin\n",
    "{bmatrix}\n",
    "y^{(1)} & y^{(2)} & \\dots y^{(60000)}\\\\\n",
    "\\end{bmatrix}$\n",
    "</center>\n",
    "<br><br>\n",
    "<center>\n",
    "$X_{test}=\\begin\n",
    "{bmatrix}\n",
    "x_{1}^{(1)} & x_{1}^{(2)} & \\dots & x_{1}^{(10000)}\\\\\n",
    "x_{2}^{(1)} & x_{2}^{(2)} & \\dots & x_{2}^{(10000)}\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "x_{784}^{(1)} & x_{784}^{(2)} & \\dots & x_{784}^{(10000)}\n",
    "\\end{bmatrix}$\n",
    "$,\\quad$\n",
    "$Y_{test}=\\begin\n",
    "{bmatrix}\n",
    "y^{(1)} & y^{(2)} & \\dots y^{(10000)}\\\\\n",
    "\\end{bmatrix}$\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 60000 examples in train set.\n",
      "There are 10000 examples in test set.\n",
      "Each example have 784 features.\n",
      "There are 10 diffrent classes.\n"
     ]
    }
   ],
   "source": [
    "m_train = X_train.shape[1]        # number of training examples\n",
    "m_test = X_test.shape[1]          # number of testing examples\n",
    "x_n = X_train.shape[0]            # number of features of each example\n",
    "classes = np.unique(Y_train).size # number of diffrent classes\n",
    "\n",
    "print(f'There are {m_train} examples in train set.')\n",
    "print(f'There are {m_test} examples in test set.')\n",
    "print(f'Each example have {x_n} features.')\n",
    "print(f'There are {classes} diffrent classes.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show example from train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is:  6\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "image = 53456\n",
    "plt.imshow(X_train[:, image].reshape(28, 28), cmap='gray')\n",
    "print('This is: ', Y_train[0, image])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2'></a>\n",
    "## 2. Utilities functions\n",
    "### 2.1 Spliting data into mini batches\n",
    "* In order to train model faster examples are splited into mini batches.<br>\n",
    "* Function ```mini_batches``` takes matrix with examples ```X```, labels ```Y``` for them, and desirable ```batch_size``` as an arguments.<br>\n",
    "* It returns list of mini batches.\n",
    "* Eachs mini batch is a tuple of two elements (examples and labels for them)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mini_batches(X, Y, batch_size=64):\n",
    "    m = X.shape[1] # number of training examples\n",
    "    mini_batches = []\n",
    "    \n",
    "    # shuffle X and Y synchronously\n",
    "    random_order = np.random.permutation(m)\n",
    "    X_shuffled = X[:, random_order]\n",
    "    Y_shuffled = Y[:, random_order]\n",
    "    \n",
    "    full_batches = m // batch_size # number of full batches\n",
    "    last_batch_size = m % batch_size # number of examples in last batch\n",
    "    \n",
    "    start = 0\n",
    "    for i in range(full_batches):\n",
    "        end = start + batch_size\n",
    "        cut_x = X_shuffled[:, start:end]\n",
    "        cut_y = Y_shuffled[:, start:end]\n",
    "        batch = (cut_x, cut_y)\n",
    "        mini_batches.append(batch)\n",
    "        start = end\n",
    "\n",
    "    if last_batch_size != 0:\n",
    "        end = start + last_batch_size\n",
    "        cut_x = X_shuffled[:, start:end]\n",
    "        cut_y = Y_shuffled[:, start:end]\n",
    "        batch = (cut_x, cut_y)\n",
    "        mini_batches.append(batch)\n",
    "    \n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2.2'></a>\n",
    "### 2.2 Create one hot vector\n",
    "* One hot vector is an diffrent representation of labels which makes implementation easier:<br><br>\n",
    "<center>\n",
    "$Y=[2,\\ 3,\\ 8,\\ 9,\\ 0,\\ 5]\\quad$ $\\Longrightarrow\\quad one\\_hot\\quad \\Longrightarrow\\quad$\n",
    "$Y_{one\\_hot}=\n",
    "\\begin\n",
    "{bmatrix}\n",
    "0&0&0&0&1&0\\\\\n",
    "0&0&0&0&0&0\\\\\n",
    "1&0&0&0&0&0\\\\\n",
    "0&1&0&0&0&0\\\\\n",
    "0&0&0&0&0&0\\\\\n",
    "0&0&0&0&0&1\\\\\n",
    "0&0&0&0&0&0\\\\\n",
    "0&0&0&0&0&0\\\\\n",
    "0&0&1&0&0&0\\\\\n",
    "0&0&0&1&0&0\\\\\n",
    "\\end{bmatrix}$\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(Y, classes):\n",
    "    Y_one_hot = np.zeros([classes, Y.shape[1]])\n",
    "    \n",
    "    for i in range(Y.shape[1]):\n",
    "        Y_one_hot[Y[0, i], i] = 1\n",
    "    \n",
    "    return Y_one_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2.3'></a>\n",
    "### 2.3 Initialize parameters\n",
    "* Initialize parameters according to neural network architecture.\n",
    "* Normal neurons are initialized as a small random number.\n",
    "* Bias neurons are initialized as zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(layers):\n",
    "    weights = {}\n",
    "    L = len(layers) # number of layers in network\n",
    "\n",
    "    for i in range(1, L):\n",
    "        W = np.random.randn(layers[i], layers[i-1]) * 0.01\n",
    "        weights['W' + str(i)] = W\n",
    "        b = np.zeros([layers[i], 1])\n",
    "        weights['b' + str(i)] = b \n",
    "    \n",
    "    return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2.4'></a>\n",
    "### 2.4 Network prediction\n",
    "* Function ```predict``` returns vector of predicted numbers for a given set of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, parameters, print_softmax=False):\n",
    "    softmax_pred, _ = L_layers_forward(X, parameters, relu, softmax)\n",
    "    if print_softmax:\n",
    "        print(softmax_pred)\n",
    "    pred = np.argmax(softmax_pred, axis=0)\n",
    "    pred = np.reshape(pred, (1, X.shape[1]))\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2.5'></a>\n",
    "### 2.5 Measure accurancy\n",
    "\n",
    "* Metrics for model performance is *accurancy*.\n",
    "* It is a ratio of correctly classified examples to all examples.\n",
    "\n",
    "<br>\n",
    "$$accurancy = \\frac{correctly\\ classified}{m}$$\n",
    "\n",
    "where:<br>\n",
    "     $\\quad m$ - number of training examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(Y_pred, Y_true):\n",
    "    accuracy = np.sum(Y_pred == Y_true) / Y_true.shape[1]\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2.6'></a>\n",
    "### 2.6 Activation functions\n",
    "#### 2.6.1 Hidden layers activation - ReLU\n",
    "\n",
    "* Rectified linear unit (ReLU) returns positive part of its argument<br>\n",
    "\n",
    "$$R(Z) = \\max(Z,\\ 0)$$\n",
    "\n",
    "<img src='drawings/relu.jpg'>\n",
    "\n",
    "<center><i>Source: medium.com/@kanchansarkar</i></center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "    A = np.maximum(0, Z)\n",
    "    cache = Z\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2.6.2'></a>\n",
    "#### 2.6.2 Output layer activation - Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Softmax fuction takes un-normalized vector of values.\n",
    "* It returns vector normalized into a probability distribution.\n",
    "* All values in returned vector are between $[0,\\ 1 ]$\n",
    "* Sum of values in normalized vector is equal to $1$, $\\big(\\sum_{i=1}^nx^{i}=1\\big)$\n",
    "<br><br><br>\n",
    "$$Softmax(Z) = \\large{\\frac{e^{z^{(i)}}}{\\sum\\limits_{i=1}^{n}e^{z^{(i)}}}}$$\n",
    "\n",
    "Legend:\n",
    "* $Z$ - vector of values\n",
    "* $n$ - number of elements in vector\n",
    "* $A$ - vector normalized into a probability distribution\n",
    "\n",
    "Example:\n",
    "\n",
    "$$Z=\\begin{bmatrix}2\\\\3\\\\9\\\\1\\\\4\\\\5\\\\\\end{bmatrix}\\quad\\Longrightarrow \\quad softmax \\quad \\Longrightarrow\n",
    "\\quad A=\\begin{bmatrix}0,0023\\\\0,0062\\\\0,9274\\\\0,0009\\\\0,0170\\\\0,0462\\\\\\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(Z):\n",
    "    A = np.exp(Z) / np.sum(np.exp(Z), axis=0, keepdims=True)\n",
    "    cache = Z\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2.6.3'></a>\n",
    "#### 2.6.3 ReLU for backward propagation\n",
    "\n",
    "* In backward propagation derivative of ReLU activation function is needed.\n",
    "* It is provided by ```relu_backward``` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_backward(dA, cache):\n",
    "    Z = cache\n",
    "    dZ = np.copy(dA)\n",
    "    dZ[Z <= 0] = 0\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3'></a>\n",
    "## 3. Model\n",
    "\n",
    "Model consists following steps:\n",
    "\n",
    "1. Choosing neural network architecture\n",
    "2. Parameters initialization\n",
    "3. Forward Propagation\n",
    "4. Computing cost\n",
    "5. Backward propagation\n",
    "6. Upadate parameters\n",
    "\n",
    "Those stepas are repeated for desirable number of times and lead to decrase in cost, by changeing parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3.1'></a>\n",
    "### 3.1  Choosing neural network architecture\n",
    "\n",
    "* Neural network architecture is contolled by python list.\n",
    "* Each number in list corresponding to the number of units in layer.\n",
    "* First element in list in a number of input units, and it's fixed because of input data structure.\n",
    "* $n_x$ = 784 - number of units in first layer $(l=0)$.\n",
    "* Last element of list is the number of units in output layer.\n",
    "* It is fixed too, because digits from $0-9$ give $10$ diffrent calsses.\n",
    "* Elements of list in between input and output layer are the number of hidden units in a given layer.\n",
    "<br>\n",
    "\n",
    "Example:\n",
    "```python\n",
    "neural_network = [784, 5, 5, 10]\n",
    "```\n",
    "Above list gives following architecture:\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<img src='drawings/net_archi.jpg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3.2'></a>\n",
    "### 3.2 Parameters initialization\n",
    "\n",
    "* In order to perform first forward propagation, first set of parameters is needed.\n",
    "* Function ```weights_init``` initialize parameters acording to the choosen neural network architecture.\n",
    "<br>\n",
    "<br>\n",
    "Example:\n",
    "\n",
    "```python\n",
    "neural_network = [784, 5, 5, 10] ==> weights_init(neural_network) ==> {W1, b1, W2, b2, W3, b3}\n",
    "```\n",
    "\n",
    "<br>\n",
    "$$W^{[1]}=\\begin{bmatrix}\n",
    "W_{1}^{(1)}&W_{1}^{(2)}&\\dots&W_{1}^{(784)}\\\\\n",
    "W_{2}^{(1)}&W_{2}^{(2)}&\\dots&W_{2}^{(784)}\\\\\n",
    "W_{3}^{(1)}&W_{3}^{(2)}&\\dots&W_{3}^{(784)}\\\\\n",
    "W_{4}^{(1)}&W_{4}^{(2)}&\\dots&W_{4}^{(784)}\\\\\n",
    "W_{5}^{(1)}&W_{5}^{(2)}&\\dots&W_{5}^{(784)}\\\\\n",
    "\\end{bmatrix}\\quad\\quad\n",
    "b^{[1]}=\\begin{bmatrix}\n",
    "b_{1}\\\\\n",
    "b_{2}\\\\\n",
    "b_{3}\\\\\n",
    "b_{4}\\\\\n",
    "b_{5}\\\\\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "<br>\n",
    "$$W^{[2]}=\\begin{bmatrix}\n",
    "W_{1}^{(1)}&W_{1}^{(2)}&W_{1}^{(3)}&W_{1}^{(4)}&W_{1}^{(5)}\\\\\n",
    "W_{2}^{(1)}&W_{2}^{(2)}&W_{2}^{(3)}&W_{2}^{(4)}&W_{2}^{(5)}\\\\\n",
    "W_{3}^{(1)}&W_{3}^{(2)}&W_{3}^{(3)}&W_{3}^{(4)}&W_{3}^{(5)}\\\\\n",
    "W_{4}^{(1)}&W_{4}^{(2)}&W_{4}^{(3)}&W_{4}^{(4)}&W_{4}^{(5)}\\\\\n",
    "W_{5}^{(1)}&W_{5}^{(2)}&W_{5}^{(3)}&W_{5}^{(4)}&W_{5}^{(5)}\\\\\n",
    "\\end{bmatrix}\\quad\\quad\n",
    "b^{[2]}=\\begin{bmatrix}\n",
    "b_{1}\\\\\n",
    "b_{2}\\\\\n",
    "b_{3}\\\\\n",
    "b_{4}\\\\\n",
    "b_{5}\\\\\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "<br>\n",
    "$$W^{[3]}=\\begin{bmatrix}\n",
    "W_{1}^{(1)}&W_{1}^{(2)}&W_{1}^{(3)}&W_{1}^{(4)}&W_{1}^{(5)}\\\\\n",
    "W_{2}^{(1)}&W_{2}^{(2)}&W_{2}^{(3)}&W_{2}^{(4)}&W_{2}^{(5)}\\\\\n",
    "\\vdots&\\vdots&\\vdots&\\vdots&\\vdots\\\\\n",
    "W_{10}^{(1)}&W_{10}^{(2)}&W_{10}^{(3)}&W_{10}^{(4)}&W_{10}^{(5)}\\\\\n",
    "\\end{bmatrix}\\quad\\quad\n",
    "b^{[3]}=\\begin{bmatrix}\n",
    "b_{1}\\\\\n",
    "b_{2}\\\\\n",
    "\\vdots\\\\\n",
    "b_{10}\\\\\n",
    "\\end{bmatrix}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3.3'></a>\n",
    "### 3.3 Forward propagation\n",
    "\n",
    "Propagation through one layer consists two steps:\n",
    "    1. Linear forward\n",
    "    2. Activation\n",
    "\n",
    " * To compute $Z^{[l]}$ in a given layer $l$ ```one_step_lin``` function is used which takes activations matrix $A^{(l-1)}$ from previous layer<br> with parameters$W^{[l]}$ and $b^{[L]}$ that controls propagation from layer $l-1$ to $l$ and uses following formula: \n",
    "\n",
    "$$Z^{[l]}=W^{[l]}A^{[l-1]}+b^{[l]}$$<br>\n",
    "\n",
    "   * Second step is done by using activation function on computetd linear part $Z^{[l]}$ whitch is done by ```one_step_lin_act``` function.\n",
    "   * Diffrent activations fucnctions can be used, but in this model $ReLU$ is used in hidden layers and $softmax$ in output layer:\n",
    "\n",
    "\n",
    "$$A^{[l]}=ReLU\\big(Z^{(l)}\\big)$$\n",
    "$$A^{[L]}=softmax\\big(Z^{(L)}\\big)$$<br>\n",
    "\n",
    "* This two esential steps are combined by ```L_layer_forward``` function, that performs propagation throug all layers,<br> and returns activations $A^{[L]}$ of output layer $L$:\n",
    "\n",
    "<br>\n",
    "<img src='drawings/forward_prop.jpg'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_step_lin(A_prev, W,  b):\n",
    "    Z = np.dot(W, A_prev) + b\n",
    "    cache = (A_prev, W, b)\n",
    "    return Z, cache\n",
    "\n",
    "\n",
    "def one_step_lin_act(A_prev, W, b, act):\n",
    "    Z, lin_cache = one_step_lin(A_prev, W, b)\n",
    "    A, act_cache = act(Z)\n",
    "    cache = (lin_cache, act_cache)\n",
    "    return A, cache\n",
    "\n",
    "\n",
    "def L_layers_forward(X, weights, act, final_L_act):\n",
    "    caches = []\n",
    "    L = len(weights) // 2 # number of layers\n",
    "    A = X\n",
    "    \n",
    "    for i in range(1, L):\n",
    "        W = weights['W' + str(i)]\n",
    "        b = weights['b' + str(i)]\n",
    "        A, cache = one_step_lin_act(A, W, b, act)\n",
    "        caches.append(cache)\n",
    "    \n",
    "    W = weights['W' + str(L)]\n",
    "    b = weights['b' + str(L)]\n",
    "    AL, cache = one_step_lin_act(A, W, b, final_L_act)\n",
    "    caches.append(cache)\n",
    "    \n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3.4'></a>\n",
    "### 3.4 Compute cost\n",
    "\n",
    "* Having activation matrix $A^{[L]}$ cost can be computed by ```compute_cost``` function according to the formula:\n",
    "\n",
    "<br>\n",
    "$$\\large{\\mathcal{J}=-\\frac{1}{m}\\sum\\limits_{i=1}^{m} \\Big[Y^{(i)}\\log\\big(A^{[L](i)}\\big) + (1-Y^{(i)})\\log\\big(1-A^{[L](i)}\\big)\\Big]}$$\n",
    "\n",
    "Legend:\n",
    "* $m$ - number of training examples\n",
    "* $Y^{(i)}$ - label for $ith$ example\n",
    "* $A^{[L](i)}$ - output of last layer for $ith$ example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_function(AL, Y):\n",
    "    m = Y.shape[1] # number of training examples\n",
    "    epsilon = 1e-8 # to prevent computing -log(0)\n",
    "    \n",
    "    cost = (1/m) * np.sum(-Y*np.log(AL+epsilon) -(1-Y)*np.log((1-AL)+epsilon))\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3.5'></a>\n",
    "### 3.5 Backward propagation\n",
    "\n",
    "* The point of backpropagation is to compute gradient of cost function with respect to the parameters $W$ and $b$.\n",
    "\n",
    "<br>\n",
    "$$dW^{[l]}=\\frac{\\partial \\mathcal{J}}{\\partial{W^{[l]}}}=\\frac{1}{m}dZ^{[l]}A^{[l-1]T}$$<br>\n",
    "$$db^{[l]}=\\frac{\\partial \\mathcal{J}}{\\partial{b^{[l]}}}=\\frac{1}{m}\\sum_{i=1}^{m}dZ^{[l](i)}$$<br>\n",
    "<center>where:</center>\n",
    "\n",
    "<br>\n",
    "$$dA^{[l-1]}=\\frac{\\partial \\mathcal{J}}{\\partial{A^{[l-1]}}}=W^{[l]T}dZ^{[l]}$$<br>\n",
    "$$dZ^{[l]}=dA^{[l]}*g\\ '\\big(Z^{[l]}\\big)$$<br>\n",
    "\n",
    "* Assuming that $dZ^{[l]}$ for a given layer $l$ is computed ```one_lin_back``` function computes $dW^{[l]}$ $db^{[l]}$ and $dA^{[l-1]}$.\n",
    "* Cache from corresponding linear step from forward propagation is needed too (functions from forward propagation steps provides it).\n",
    "\n",
    "<br>\n",
    "<img src='drawings/ss_back.jpg'>\n",
    "\n",
    "<br>\n",
    "\n",
    "* In order to perform above step $dZ^{[l]}$ is needed which is computed as follows:\n",
    "\n",
    "<br>\n",
    "$$dZ^{[l]}=dA^{[l]}*g\\ '\\big(Z^{[l]}\\big)$$<br>\n",
    "\n",
    "* $g\\ '\\big(Z^{[l]}\\big)$ is a deriviative of activation function used in layer $l$.\n",
    "* For ReLU - ```relu_backward``` function:\n",
    "\n",
    "<br>\n",
    "$$ReLU'(z)=\\begin{cases}\n",
    "1&\\text{if}\\ z>0\\\\\n",
    "0&\\text{otherwise}\n",
    "\\end{cases}$$<br>\n",
    "\n",
    "* Function ```one_lin_act_back``` combines this two steps.\n",
    "* It takes $dA^{[l]}$, $cache^{[l]}$ and $activation^{[l]}$ as an arguments, than computes $dZ$ and uses ```one_lin_back```\n",
    "function to get $dW^{[l]}$, $db^{[l]}$ and $dA^{[l-1]}$.\n",
    "* ```L_layers_backprop``` function takes care of repeating this procedure for enough times to back propagate through all network and intialize whole process.\n",
    "* To start back propagation $dZ$ for last layer is needed. This model uses on output layer $softmax$ activation function for which $dZ$ is computed as follows:<br>\n",
    "\n",
    "$$dZ^{[L]}=A^{[L]}-Y$$\n",
    "\n",
    "Where:\n",
    "* $Y$ - true labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_lin_back(dZ, cache):\n",
    "    A_prev, W, b = cache # data from correspondig step in forwad propagation\n",
    "    m = A_prev.shape[1]  # number of examples\n",
    "    \n",
    "    dW = np.dot(dZ, A_prev.T) / m\n",
    "    db = np.sum(dZ, axis = 1, keepdims = True) / m\n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "    \n",
    "    return dA_prev, dW, db\n",
    "\n",
    "\n",
    "def one_lin_act_back(dA, cache, activation):\n",
    "    linear_cache, activation_cache = cache # data from correspondig step in forwad propagation\n",
    "    \n",
    "    dZ = activation(dA, activation_cache)\n",
    "    dA_prev, dW, db = one_lin_back(dZ, linear_cache)\n",
    "    \n",
    "    return dA_prev, dW, db\n",
    "\n",
    "\n",
    "def L_layers_backprop(AL, Y, caches):\n",
    "    grads = {}\n",
    "    L = len(caches) # number of layers\n",
    "    m = AL.shape[1] # number of examples\n",
    "    \n",
    "    # initialization of backprop for softmax\n",
    "    linear_cache, _ = caches[L-1]\n",
    "    dZL = AL - Y\n",
    "    dA_prev, dW, db = one_lin_back(dZL, linear_cache)\n",
    "    grads['dA' + str(L - 1)], grads['dW' + str(L)], grads['db' + str(L)] = dA_prev, dW, db\n",
    "  \n",
    "    # rest layers\n",
    "    for l in reversed(range(L-1)):\n",
    "        current_cache = caches[l] # data from correspondig step in forwad propagation\n",
    "        dA_prev_temp, dW_temp, db_temp = one_lin_act_back(grads[\"dA\" + str(l + 1)], current_cache, relu_backward)\n",
    "        grads[\"dA\" + str(l)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "        grads[\"db\" + str(l + 1)] = db_temp\n",
    "        \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3.6'></a>\n",
    "### 3.6 Update parameters (Adam optimizer)\n",
    "\n",
    "* Having partial derivatives of parameters for each layer $(dW$, $db)$ parameters can be updated to reduce cost.\n",
    "* It is done by Adam optimizer - ```update_adam``` function which is performing following steps:\n",
    "    1. It calculates exponentially weighted average of past gradients and stores it in variable $\\upsilon$ (before bias correction) and $\\upsilon^{corrected}$ (after bias correction).\n",
    "    2. It does the same thing for the squares of exponentialy weighted average of past gradients - $s$ and $s^{(corrected)}$.\n",
    "    3. Informations from step \"1\" and \"2\" are used to update parameters for every layers.<br>\n",
    "    \n",
    "    \n",
    "* Adam optimizer formulas:\n",
    "\n",
    "<br>\n",
    "$$\\large{\\upsilon_{dW^{[l]}}=\\beta_{1}\\upsilon_{dW^{[l]}}+\\left(1-\\beta_{1}\\right)\\frac{\\partial\\mathcal{J}}{\\partial W^{[l]}}}$$\n",
    "\n",
    "$$\\large{\\upsilon_{dW^{[l]}}^{corrected}=\\frac{dW^{[l]}}{1-\\left(\\beta_{1}\\right)^t}}$$\n",
    "\n",
    "$$\\large{s_{dW^{[l]}}=\\beta_{2}s_{dW^{[l]}}+(1-\\beta_{2})\\left(\\frac{\\partial\\mathcal{J}}{\\partial W^{[l]}}\\right)^{2}}$$\n",
    "\n",
    "$$\\large{s_{dW^{[l]}}^{corrected}=\\frac{dW^{[l]}}{1-(\\beta_{2})^t}}$$\n",
    "\n",
    "$$\\large{W^{[l]}=W^{[l]}-\\alpha\\frac{\\upsilon_{dW^{[l]}}^{corrected}}{\\sqrt{s_{dW^{[l]}}^{corrected}}+\\epsilon}}$$\n",
    "\n",
    "Legend:\n",
    "\n",
    "* $\\beta_1$ and $\\beta_2$ - hyperparameters that control the two exponentially weighted averages\n",
    "* $t$ - number of steps taken \n",
    "* $\\alpha$ - learning rate\n",
    "* $\\epsilon$ - samll number to avioid dividing by $0$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_adam(weights, grads, v, s, t, learning_rate=0.01, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "    L = len(weights) // 2 \n",
    "    v_corrected = {}\n",
    "    s_corrected = {}\n",
    "    \n",
    "    for l in range(L):\n",
    "        v['dW' + str(l+1)] = beta1 * v['dW' + str(l+1)] + (1-beta1) * grads['dW' + str(l+1)]\n",
    "        v['db' + str(l+1)] = beta1 * v['db' + str(l+1)] + (1-beta1) * grads['db' + str(l+1)]\n",
    "        \n",
    "        v_corrected[\"dW\" + str(l+1)] = v[\"dW\" + str(l+1)] / (1 - beta1**t)\n",
    "        v_corrected[\"db\" + str(l+1)] = v[\"db\" + str(l+1)] / (1 - beta1**t)\n",
    "        \n",
    "        s[\"dW\" + str(l+1)] = beta2 * s[\"dW\" + str(l+1)] + (1 - beta2) * grads[\"dW\" + str(l+1)]**2\n",
    "        s[\"db\" + str(l+1)] = beta2 * s[\"db\" + str(l+1)] + (1 - beta2) * grads[\"db\" + str(l+1)]**2\n",
    "        \n",
    "        s_corrected[\"dW\" + str(l+1)] = s[\"dW\" + str(l+1)] / (1 - beta2**t)\n",
    "        s_corrected[\"db\" + str(l+1)] = s[\"db\" + str(l+1)] / (1 - beta2**t)\n",
    "        \n",
    "        weights[\"W\" + str(l+1)] = weights[\"W\" + str(l+1)] - (learning_rate * v_corrected[\"dW\" + str(l+1)]) / (np.sqrt(s_corrected[\"dW\" + str(l+1)]) + epsilon)\n",
    "        weights[\"b\" + str(l+1)] = weights[\"b\" + str(l+1)] - (learning_rate * v_corrected[\"db\" + str(l+1)]) / (np.sqrt(s_corrected[\"db\" + str(l+1)]) + epsilon)\n",
    "        \n",
    "    return weights, v, s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3.6.1'></a>\n",
    "#### 3.6.1 Parameters initialization for Adam optimizer\n",
    "\n",
    "* In order to start Adam optimizer his $\\upsilon$ and $s$ variables need to be initialized.\n",
    "* Function ```adam_init``` takes dictionary with parameters and returns $\\upsilon$ and $s$ in the same shape as parameters with values equals zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adam_init(weights):\n",
    "    L = len(weights) // 2 # number of layers\n",
    "    v = {}\n",
    "    s = {}\n",
    "    \n",
    "    for l in range(L):\n",
    "        v['dW' + str(l+1)] = np.zeros(weights['W' + str(l+1)].shape)\n",
    "        v['db' + str(l+1)] = np.zeros(weights['b' + str(l+1)].shape)\n",
    "        \n",
    "        s['dW' + str(l+1)] = np.zeros(weights['W' + str(l+1)].shape)\n",
    "        s['db' + str(l+1)] = np.zeros(weights['b' + str(l+1)].shape)\n",
    "        \n",
    "    return v, s  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3.7'></a>\n",
    "### 3.7 Final model function\n",
    "\n",
    "* Function ```model``` puts together all neccesery steps to train neural network.\n",
    "* Training is done on mini batches for desireable number of epochs.\n",
    "* After each epoch cost is printed and after whole trainig plot of cost in function of epochs is returned. \n",
    "\n",
    "<br>\n",
    "<img src='drawings/model_schematic.jpg'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X, Y, layers_dims, learning_rate=0.0075, num_epochs=10, batch_size=64,\n",
    "          beta1=0.9, beta2=0.999, epsilon=1e-8, print_cost=True):\n",
    "    \n",
    "    costs = []\n",
    "    weights = weights_init(layers_dims)\n",
    "    v, s = adam_init(weights)\n",
    "    start = time.time() # for measuring training time\n",
    "    t = 0 # counter for Adam optimizer\n",
    "    \n",
    "    # compute and print initial cost\n",
    "    AL, caches = L_layers_forward(X, weights, relu, softmax)\n",
    "    init_cost = cost_function(AL, Y)\n",
    "    costs.append(init_cost)\n",
    "    print('Initial cost:', init_cost)\n",
    "\n",
    "    for i in range(num_epochs):\n",
    "        batches = mini_batches(X, Y, batch_size)\n",
    "        \n",
    "        for j in range(len(batches)):\n",
    "            x = batches[j][0]\n",
    "            y = batches[j][1]\n",
    "            AL, caches = L_layers_forward(x, weights, relu, softmax)\n",
    "            cost = cost_function(AL, y)\n",
    "            grads = L_layers_backprop(AL, y, caches)\n",
    "            t = t+1 # Adam counter\n",
    "            weights, v, s = update_adam(weights, grads, v, s, t, learning_rate, beta1, beta2, epsilon)\n",
    "            \n",
    "        if print_cost and i % 1 == 0:\n",
    "            print (\"Cost after epoch %i: %f\" %(i+1, cost))\n",
    "        if print_cost and i % 1 == 0:\n",
    "            costs.append(cost)\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    \n",
    "    ax.plot(np.squeeze(costs))\n",
    "    ax.set_xlabel('epoch').set_color('red')\n",
    "    ax.set_ylabel('cost').set_color('red')\n",
    "    ax.set_title(\"Learning rate =\" + str(learning_rate)).set_color('red')\n",
    "    ax.tick_params(axis='x', colors='red')\n",
    "    ax.tick_params(axis='y', colors='red')\n",
    "    \n",
    "    \n",
    "    end = time.time()\n",
    "    seconds = int(np.round((end - start)))\n",
    "    minutes = (seconds // 60) % 60\n",
    "    hours = (seconds // 60) // 60\n",
    "    seconds = seconds - 60*minutes - 60*60*hours\n",
    "    print(' ')\n",
    "    print(f'{num_epochs} epochs took {hours}h {minutes}m {seconds}s')\n",
    "    \n",
    "    return weights, costs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='4'></a>\n",
    "## 4 Training\n",
    "### 4.1 Hyperparameters\n",
    "\n",
    "* Following hyperpareameters was used to train neural network:\n",
    "    * 4 layers dense neural network $(64,\\ 64,\\ 64,\\ 10)$\n",
    "    * learning rate $\\alpha = 0.0001$\n",
    "    * mini-batch size $=128$\n",
    "    * number of epochs $=200$\n",
    "    * $\\beta_{1} = 0.9$ \n",
    "    * $\\beta_{2} = 0.999$\n",
    "    * $\\varepsilon = 1^{-8}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_train\n",
    "Y = one_hot(Y_train, 10)\n",
    "layers = [X_train.shape[0], 64, 64, 64, 10]\n",
    "alpha = 0.0001\n",
    "mini_batch_size = 128\n",
    "epochs = 200\n",
    "beta1 = 0.9\n",
    "beta2 = 0.999\n",
    "epsilon = 1e-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial cost: 3.250828493819358\n",
      "Cost after epoch 1: 2.012192\n",
      "Cost after epoch 2: 1.159962\n",
      "Cost after epoch 3: 1.063896\n",
      "Cost after epoch 4: 0.943020\n",
      "Cost after epoch 5: 0.875432\n",
      "Cost after epoch 6: 0.903046\n",
      "Cost after epoch 7: 0.612817\n",
      "Cost after epoch 8: 0.847963\n",
      "Cost after epoch 9: 0.817528\n",
      "Cost after epoch 10: 0.864686\n",
      "Cost after epoch 11: 0.574609\n",
      "Cost after epoch 12: 0.674983\n",
      "Cost after epoch 13: 0.682500\n",
      "Cost after epoch 14: 0.581293\n",
      "Cost after epoch 15: 0.589540\n",
      "Cost after epoch 16: 0.698072\n",
      "Cost after epoch 17: 0.380560\n",
      "Cost after epoch 18: 0.745633\n",
      "Cost after epoch 19: 0.382566\n",
      "Cost after epoch 20: 0.453000\n",
      "Cost after epoch 21: 0.365241\n",
      "Cost after epoch 22: 0.393345\n",
      "Cost after epoch 23: 0.349892\n",
      "Cost after epoch 24: 0.578434\n",
      "Cost after epoch 25: 0.351224\n",
      "Cost after epoch 26: 0.543041\n",
      "Cost after epoch 27: 0.089921\n",
      "Cost after epoch 28: 0.541142\n",
      "Cost after epoch 29: 0.296596\n",
      "Cost after epoch 30: 0.296756\n",
      "Cost after epoch 31: 0.150922\n",
      "Cost after epoch 32: 0.343655\n",
      "Cost after epoch 33: 0.320917\n",
      "Cost after epoch 34: 0.206742\n",
      "Cost after epoch 35: 0.150046\n",
      "Cost after epoch 36: 0.113737\n",
      "Cost after epoch 37: 0.227911\n",
      "Cost after epoch 38: 0.165436\n",
      "Cost after epoch 39: 0.282632\n",
      "Cost after epoch 40: 0.305628\n",
      "Cost after epoch 41: 0.173125\n",
      "Cost after epoch 42: 0.145162\n",
      "Cost after epoch 43: 0.130447\n",
      "Cost after epoch 44: 0.316902\n",
      "Cost after epoch 45: 0.286356\n",
      "Cost after epoch 46: 0.242492\n",
      "Cost after epoch 47: 0.107412\n",
      "Cost after epoch 48: 0.274388\n",
      "Cost after epoch 49: 0.153530\n",
      "Cost after epoch 50: 0.143578\n",
      "Cost after epoch 51: 0.273477\n",
      "Cost after epoch 52: 0.061134\n",
      "Cost after epoch 53: 0.245354\n",
      "Cost after epoch 54: 0.197098\n",
      "Cost after epoch 55: 0.121482\n",
      "Cost after epoch 56: 0.087719\n",
      "Cost after epoch 57: 0.143235\n",
      "Cost after epoch 58: 0.152717\n",
      "Cost after epoch 59: 0.085226\n",
      "Cost after epoch 60: 0.139258\n",
      "Cost after epoch 61: 0.152098\n",
      "Cost after epoch 62: 0.120457\n",
      "Cost after epoch 63: 0.065445\n",
      "Cost after epoch 64: 0.079364\n",
      "Cost after epoch 65: 0.130407\n",
      "Cost after epoch 66: 0.128899\n",
      "Cost after epoch 67: 0.049728\n",
      "Cost after epoch 68: 0.122357\n",
      "Cost after epoch 69: 0.248958\n",
      "Cost after epoch 70: 0.065514\n",
      "Cost after epoch 71: 0.073489\n",
      "Cost after epoch 72: 0.041681\n",
      "Cost after epoch 73: 0.119019\n",
      "Cost after epoch 74: 0.115178\n",
      "Cost after epoch 75: 0.066759\n",
      "Cost after epoch 76: 0.148347\n",
      "Cost after epoch 77: 0.116165\n",
      "Cost after epoch 78: 0.123513\n",
      "Cost after epoch 79: 0.108372\n",
      "Cost after epoch 80: 0.179144\n",
      "Cost after epoch 81: 0.159913\n",
      "Cost after epoch 82: 0.187646\n",
      "Cost after epoch 83: 0.149680\n",
      "Cost after epoch 84: 0.052478\n",
      "Cost after epoch 85: 0.144275\n",
      "Cost after epoch 86: 0.163385\n",
      "Cost after epoch 87: 0.210155\n",
      "Cost after epoch 88: 0.071966\n",
      "Cost after epoch 89: 0.109699\n",
      "Cost after epoch 90: 0.122599\n",
      "Cost after epoch 91: 0.127567\n",
      "Cost after epoch 92: 0.057057\n",
      "Cost after epoch 93: 0.133636\n",
      "Cost after epoch 94: 0.017967\n",
      "Cost after epoch 95: 0.114488\n",
      "Cost after epoch 96: 0.050962\n",
      "Cost after epoch 97: 0.110299\n",
      "Cost after epoch 98: 0.127284\n",
      "Cost after epoch 99: 0.120102\n",
      "Cost after epoch 100: 0.016736\n",
      "Cost after epoch 101: 0.127921\n",
      "Cost after epoch 102: 0.100080\n",
      "Cost after epoch 103: 0.242728\n",
      "Cost after epoch 104: 0.056389\n",
      "Cost after epoch 105: 0.106883\n",
      "Cost after epoch 106: 0.247202\n",
      "Cost after epoch 107: 0.050018\n",
      "Cost after epoch 108: 0.053499\n",
      "Cost after epoch 109: 0.032213\n",
      "Cost after epoch 110: 0.126449\n",
      "Cost after epoch 111: 0.037301\n",
      "Cost after epoch 112: 0.079060\n",
      "Cost after epoch 113: 0.125381\n",
      "Cost after epoch 114: 0.044063\n",
      "Cost after epoch 115: 0.166823\n",
      "Cost after epoch 116: 0.062024\n",
      "Cost after epoch 117: 0.070314\n",
      "Cost after epoch 118: 0.066940\n",
      "Cost after epoch 119: 0.013154\n",
      "Cost after epoch 120: 0.048625\n",
      "Cost after epoch 121: 0.054371\n",
      "Cost after epoch 122: 0.014951\n",
      "Cost after epoch 123: 0.102828\n",
      "Cost after epoch 124: 0.093803\n",
      "Cost after epoch 125: 0.182010\n",
      "Cost after epoch 126: 0.019037\n",
      "Cost after epoch 127: 0.058512\n",
      "Cost after epoch 128: 0.022093\n",
      "Cost after epoch 129: 0.122288\n",
      "Cost after epoch 130: 0.043643\n",
      "Cost after epoch 131: 0.040028\n",
      "Cost after epoch 132: 0.015159\n",
      "Cost after epoch 133: 0.076121\n",
      "Cost after epoch 134: 0.024183\n",
      "Cost after epoch 135: 0.121871\n",
      "Cost after epoch 136: 0.180442\n",
      "Cost after epoch 137: 0.040709\n",
      "Cost after epoch 138: 0.011049\n",
      "Cost after epoch 139: 0.028234\n",
      "Cost after epoch 140: 0.025948\n",
      "Cost after epoch 141: 0.023443\n",
      "Cost after epoch 142: 0.014414\n",
      "Cost after epoch 143: 0.096727\n",
      "Cost after epoch 144: 0.038698\n",
      "Cost after epoch 145: 0.044527\n",
      "Cost after epoch 146: 0.027803\n",
      "Cost after epoch 147: 0.018722\n",
      "Cost after epoch 148: 0.048799\n",
      "Cost after epoch 149: 0.014916\n",
      "Cost after epoch 150: 0.052514\n",
      "Cost after epoch 151: 0.007723\n",
      "Cost after epoch 152: 0.027921\n",
      "Cost after epoch 153: 0.014705\n",
      "Cost after epoch 154: 0.097434\n",
      "Cost after epoch 155: 0.099742\n",
      "Cost after epoch 156: 0.037085\n",
      "Cost after epoch 157: 0.011447\n",
      "Cost after epoch 158: 0.017336\n",
      "Cost after epoch 159: 0.008349\n",
      "Cost after epoch 160: 0.022799\n",
      "Cost after epoch 161: 0.020141\n",
      "Cost after epoch 162: 0.046924\n",
      "Cost after epoch 163: 0.004785\n",
      "Cost after epoch 164: 0.008066\n",
      "Cost after epoch 165: 0.005235\n",
      "Cost after epoch 166: 0.033590\n",
      "Cost after epoch 167: 0.005133\n",
      "Cost after epoch 168: 0.018315\n",
      "Cost after epoch 169: 0.017954\n",
      "Cost after epoch 170: 0.006344\n",
      "Cost after epoch 171: 0.040760\n",
      "Cost after epoch 172: 0.003405\n",
      "Cost after epoch 173: 0.016310\n",
      "Cost after epoch 174: 0.001358\n",
      "Cost after epoch 175: 0.007556\n",
      "Cost after epoch 176: 0.018899\n",
      "Cost after epoch 177: 0.003194\n",
      "Cost after epoch 178: 0.011102\n",
      "Cost after epoch 179: 0.099191\n",
      "Cost after epoch 180: 0.007208\n",
      "Cost after epoch 181: 0.002726\n",
      "Cost after epoch 182: 0.005147\n",
      "Cost after epoch 183: 0.008287\n",
      "Cost after epoch 184: 0.021190\n",
      "Cost after epoch 185: 0.032572\n",
      "Cost after epoch 186: 0.013084\n",
      "Cost after epoch 187: 0.004918\n",
      "Cost after epoch 188: 0.001530\n",
      "Cost after epoch 189: 0.010740\n",
      "Cost after epoch 190: 0.039212\n",
      "Cost after epoch 191: 0.009731\n",
      "Cost after epoch 192: 0.010298\n",
      "Cost after epoch 193: 0.005776\n",
      "Cost after epoch 194: 0.020809\n",
      "Cost after epoch 195: 0.011314\n",
      "Cost after epoch 196: 0.052427\n",
      "Cost after epoch 197: 0.008201\n",
      "Cost after epoch 198: 0.030999\n",
      "Cost after epoch 199: 0.005502\n",
      "Cost after epoch 200: 0.004816\n",
      " \n",
      "200 epochs took 0h 8m 11s\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "parameters, cost = model(X, Y, layers, alpha, epochs, mini_batch_size, beta1, beta2, epsilon, print_cost=True)\n",
    "\n",
    "# save parameters\n",
    "f = open('64_64_64.pkl', 'wb')\n",
    "pickle.dump(parameters, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='4.2'></a>\n",
    "### 4.2 Model performace\n",
    "\n",
    "* Model performance is measured by *accurancy* metrics on *train set* and *test set*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on train set:  0.9994\n",
      "Accuracy on test set:  0.9583\n"
     ]
    }
   ],
   "source": [
    "# load pre trained parameters\n",
    "f = open('64_64_64.pkl', 'rb')\n",
    "parameters = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "pred_train = predict(X, parameters)\n",
    "pred_test = predict(X_test, parameters)\n",
    "\n",
    "train_acc = accuracy(pred_train, Y_train)\n",
    "test_acc = accuracy(pred_test, Y_test)\n",
    "print(\"Accuracy on train set: \", train_acc)\n",
    "print(\"Accuracy on test set: \", test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='5'></a>\n",
    "## 5. Image prediction \n",
    "\n",
    "* Parameters acquired during training can be used to recognize real world handwritten digits.\n",
    "* Function ```image_predict``` does that; it takes as an arguments:\n",
    "    * image name in string - photo of digit that should be in *example_digits* folder\n",
    "    * true label of the photo (int)\n",
    "    * parameters from training\n",
    "    * *print_softmax* - allows to print whole output of softmax function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_predict(image, image_label, parameters, print_softmax=False):\n",
    "    file_name = 'example_digits/' + image\n",
    "\n",
    "    image_arr = -imageio.imread(file_name, as_gray=True)\n",
    "    image_arr = image_arr + 255 # to make background black\n",
    "    image_arr = image_arr / 255 # normalize values to be betweeen 0 and 1\n",
    "\n",
    "    image_resized = resize(image_arr, (28, 28), anti_aliasing=False, mode='reflect')\n",
    "    image_vec = image_resized.reshape((784, 1))\n",
    "\n",
    "    prediction = predict(image_vec, parameters, print_softmax)\n",
    "\n",
    "    print('Network prediction: ', np.squeeze(prediction))\n",
    "    print('True label: ', image_label)\n",
    "\n",
    "    if (prediction == image_label):\n",
    "        print('Network recognized digit correctly!')\n",
    "    else:\n",
    "        print('Too bad!')\n",
    "\n",
    "    im = plt.imshow(image_resized, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='5.1'></a>\n",
    "### 5.1 Example\n",
    "\n",
    "* The *example_images* folder contains sample digits written by me.\n",
    "* As it can be seen network is often wrong, it is probably caused by the noisy background of real world photos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network prediction:  2\n",
      "True label:  4\n",
      "Too bad!\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "image_predict('four.jpg', 4, parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "* After removing backgruound predictions are better, but not always."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network prediction:  4\n",
      "True label:  4\n",
      "Network recognized digit correctly!\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "image_predict('four_clean.jpg', 4, parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='6'></a>\n",
    "## 6. Conclusion\n",
    "\n",
    "* The model has been successfully implemented and it works.\n",
    "* Accuracy on the *train set* was $99.94\\%$ and on the *test set* $95.83\\%$.\n",
    "* Gap between train set and test set is equal $4.11\\%$ and accuracy on train set is almost $100\\%$ whitch might indicate that model is overfitting.\n",
    "* Network does not perform well on real world images.\n",
    "* Next steps:\n",
    "    * Adding some form of regularization, eg *Dropout* or *L₂ Regularization*.\n",
    "    * Using Data Augmentation or training on more realistic data set could make model perform better on real world digits.\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "*This project was inspired by Deep Learning Specialization Courses by Andrew Ng on Coursera.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
